# Basic Notions of Probability
:::success
[å›éš¨æ©Ÿå¾®ç©åˆ†](/4m94zjAmQYSZ6dupjdsNRw)
ç·¨è¼¯:2023/04/09
æ­¤ç­†è¨˜çš„ipyndæª”å¯ä»¥åœ¨æˆ‘çš„[Github](https://github.com/Iofting1023)æ‰¾åˆ°ï¼
:::
[TOC]
## 1.1. Distributions as histograms.
#### (a) 
Sample $ğ‘ = 10,000$ random numbers uniformly on $[0, 1]$.
```python=+
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(1023)
#a
N = 10000
sample = np.random.rand(N)
sample
```
```
array([0.59379366, 0.85444082, 0.03805208, ..., 0.85521843, 0.10999541,
       0.67130031])
```
#### (b) 
Plotting the PDF: Divide the interval $[0,1]$ into $ğ‘š$ bins $ğµ_1,\ldots,ğµ_{50}$ of equal length $1/50$. Plot the histogram of the values for $50$ bins where the value at bin $j$ is $$\frac{\#\{i\leq N:X_i\in B_j\}}{N}$$
```python=+
plt.hist(sample , density =True,bins =50)
plt.show()
```
![image](https://hackmd.io/_uploads/H1UPjjZgA.png)

#### (c)
Plotting the CDF: Plot the cumulative histogram of the values for $50$ bins where the value at each bin $ğ‘—$ is
$$\frac{\#\{i\leq N:X_i\leq j/50\}}{N}$$
```python=+
#c
plt.hist(sample , density =True,bins =50, cumulative = True)
plt.show()
```
![image](https://hackmd.io/_uploads/BJ3yhsWx0.png)

#### (d)
Re-do items (b) and (c) for the square of each number. This would approximate the PDF of $X^2$ where ğ‘‹ is uniformly distributed on $[0, 1]$.
```python=+
sample_2 = sample**2
plt.hist(sample_2 , density =True,bins =50)
plt.show()
plt.hist(sample_2 , density =True,bins =50, cumulative = True)
plt.show()
```
![image](https://hackmd.io/_uploads/BJzUhiZlR.png)
![image](https://hackmd.io/_uploads/ByLD3obeA.png)

## 1.2. The law of large numbers.
#### (a)The strong law.
Sample $ğ‘ = 10,000$ random numbers $ğ‘‹_1,\ldots, ğ‘‹_ğ‘$ exponentially distributed with parameter 1. Use the command ``numpy.cumsum`` to
get the empirical mean $\frac{ğ‘†_ğ‘}{N} = \frac{1}{N}(ğ‘‹_1+\dots+ğ‘‹_ğ‘)$.Plot the values for $ğ‘=
1,\dots, 10,000.$ What do you notice?
$$P(\lim_{n\rightarrow \infty}\frac{S_N}{N}=1)=1$$
æ­¤æŒ‡æ•¸åˆ†é…çš„æœŸæœ›å€¼ç‚º1
```python=+
N = 10000
sample = np.random.exponential(1,N)
S_n = [np.cumsum(sample)[i]/(i+1) for i in range(10000)]
plt.plot(S_n)
```
![image](https://hackmd.io/_uploads/Sk0Xl2WeA.png)
#### (b)The weak law.
Define a function in Python using the command ``def`` that returns the empirical mean of a sample of size $ğ‘$ as above. This will allow you to sample the empirical mean $\frac{ğ‘†_ğ‘}{N}$ as many times as needed for a given $ğ‘$.Plot the histograms (PDF and CDF) of a sample of size 10,000 of the empirical
mean for $ğ‘ = 100$ and $ğ‘ = 10,000$. What do you notice?
```python=+
def sample_mean(N,times):
    mean = []
    for i in range(times):
        mean.append(np.random.exponential(1,N).mean())
    return mean
t_100 = sample_mean(100,10000)
t_10000 = sample_mean(10000,10000)
plt.hist(t_100, density = True)
plt.hist(t_10000, density = True)
plt.show()
```
![image](https://hackmd.io/_uploads/Hklzz3-l0.png)
èˆ‡ä¸Šä¸€å°é¡Œä¸åŒçš„æ˜¯(a)å°é¡Œæ˜¯ç›´æ¥ç•«å‡º$\frac{S_N}{N}$éš¨$N$è®Šå¤§å€¼çš„è®ŠåŒ–ï¼Œé€™ä¸€å°é¡Œæ˜¯çœ‹çµ¦å®š$N$,$\frac{S_N}{N}$æŠ½æ¨£è¨±å¤šæ¬¡çš„å€¼ï¼Œæƒ³æè¿°çš„æ˜¯å¤§æ•¸å¼±æ³•å‰‡$$\lim_{n\rightarrow \infty}P(\frac{S_N}{N}=1)=1$$
å¯ä»¥é æœŸçš„æ˜¯ç•¶$N$ä¸Šå‡ï¼ŒæŠ½æ¨£å‡ºçš„æ¨£æœ¬å€¼é›¢1è¶Šä¾†è¶Šè¿‘ï¼Œä¹Ÿå°±æ˜¯åˆ†ä½ˆæ”¶æ–‚åˆ°å–®é»1ï¼ŒåŒæ™‚ä¹Ÿè¡¨ç¤ºæ©Ÿç‡æ”¶æ–‚åˆ°1.
## 1.3 The central limit theorem
The approximation of the expectation in terms of the empirical mean is not exact for finite $ğ‘$. The error is controlled by the central limit theorem. For a sample $ğ‘‹_1 , \ldots , ğ‘‹_ğ‘$ of the random variable $ğ‘‹$ of mean $E[ğ‘‹ ]$ and variance $\sigma^2$, this theorem says that the sum $ğ‘†_ğ‘ = ğ‘‹_1 +\ldots+ ğ‘‹_ğ‘$ behaves like $ğ‘†_ğ‘ \approx ğ‘E[ğ‘‹]+\sqrt{ğ‘}\sigmağ‘$, where $ğ‘$ is a standard Gaussian random variable. More precisely, this means that
$$\lim_{n\rightarrow \infty}\frac{S_N-NE[X]}{\sigma\sqrt{N}}=Z$$
The limit should be understood here as the convergence in distribution. Practically speaking, this means that the histogram of the random variable $\frac{ğ‘†_ğ‘âˆ’ğ‘E[ğ‘‹]}{\sigma\sqrt{N}}$ should resemble the one of a standard Gaussian variable when $ğ‘$ is large.We check this numerically.
#### (a)
Let $ğ‘†_ğ‘ = ğ‘‹_1 +\ldots + ğ‘‹_ğ‘$ where the $ğ‘‹_ğ‘–$ are exponentially distributed random variables of parameter $1$. Define a function in Python using the command ``def`` that returns for a given $ğ‘$ the value $ğ‘Œ_ğ‘ =\frac{ğ‘†_ğ‘âˆ’ğ‘E[ğ‘‹]}{\sigma\sqrt{N}}$
```python=+
def y_n(N):
    sample = np.random.exponential(1,N)
    standard = (sample.sum()-N*1)/1
    return standard
```

#### (b)
Plot the histograms (PDF) of a sample of size 10,000 of $ğ‘Œ_ğ‘$ for $ğ‘ = 100$. What do you notice?
```python=+
#b
N = 100
size = 10000
sample = [y_n(N) for i in range(size)]
plt.hist(sample , density =True,bins =50)
plt.show()
```
![image](https://hackmd.io/_uploads/SktLdUGe0.png)
ä¹çœ‹è·Ÿå¸¸æ…‹å·®ä¸å¤š
#### (c)
Compare the above to the histogram of a sample of size $10,000$ of points generated using the standard Gaussian distribution.
```python=+
sample = np.random.normal(0,1,10000)
plt.hist(sample, density =True,bins =50)
plt.show()
```
![image](https://hackmd.io/_uploads/SkJyK8fx0.png)
è·Ÿbå°é¡Œå½¢ç‹€å¾ˆåƒï¼
## 1.4 Sampling Cauchy random variables.
shows that there is no law of large numbers (weak or strong) for Cauchy random variables.
#### (a)
Let $ğ¹_X^{âˆ’1}$ be the inverse of the CDF of the Cauchy distribution. Plot the histogram of $ğ¹_X^{âˆ’1}(ğ‘ˆ)$ where $ğ‘ˆ$ is a uniform random variable for $ğ‘‹$ sample of $10,000$ points. Use $100$ bins in the interval $[-10,10]$.
- First compute $F^{-1}_X$, since $F_X(x)=\frac{1}{\pi}\tan^{-1}x+\frac{1}{2}$, we can find $F^{-1}_X(x)=\tan(\pi(x-\frac{1}{2}))$
```python=+
N = 10000
uni_sample = np.random.rand(N)
cauchy_sample = np.tan(np.pi*(uni_sample - 0.5))
plt.hist(cauchy_sample, density =True,bins =100,range=[-10,10])
plt.show()
```
![image](https://hackmd.io/_uploads/Hkfyn8GxA.png)
#### (b)
Compare the above to the histogram of a sample of size $10,000$ of points generated using the standard Gaussian distribution.
```python=+
#b
sample = np.random.normal(0,1,N)
plt.hist(sample, density =True,bins =100,range=[-10,10])
plt.show()
```
![image](https://hackmd.io/_uploads/HkA72IGlC.png)
å¯ä»¥æ˜é¡¯çœ‹å‡ºå¾å¸¸æ…‹æŠ½æ¨£å°¾ç«¯äº‹ä»¶çš„æ¨£æœ¬å°‘å¾ˆå¤šï¼Œå¹¾ä¹æ˜¯æ²’æœ‰ï¼
#### (c)
 Let ($ğ¶_ğ‘›,ğ‘›\leq 10,000$) be the values obtained in (a). Plot the empirical mean $\frac{ğ‘†_ğ‘}{N} = \frac{1}{N} \sum_{n\leq N} ğ¶_ğ‘›$ for $ğ‘ = 1,\ldots,10,000$. What do you notice?
```python=+
#c
S_n = [np.cumsum(cauchy_sample)[i]/(i+1) for i in range(10000)]
plt.plot(S_n)
```
![image](https://hackmd.io/_uploads/B1hbCIzeA.png)
æ¨£æœ¬å¹³å‡ä¸¦æ²’æœ‰éš¨è‘—$N$(æŠ½æ¨£æ•¸è®Šå¤§)æœ‰æ”¶æ–‚åˆ°å®šå€¼çš„è¶¨å‹¢ï¼
#### (d)
Define a function in Python using the command ``def`` that returns the empirical mean $\frac{ğ‘†_ğ‘}{N}$ of a sample of size $ğ‘$ as above. Plot the histograms (PDF) of a $ğ‘$ sample of size $10,000$ of the empirical mean for $ğ‘ = 10$ and $ğ‘ = 100$. What do you notice compared to the histograms in Project 1.2?
```python=+
size = 10000
def cauchy_sample_mean(N,size):
    mean = []
    for i in range(size):
        uni_sample = np.random.rand(N)
        cauchy_mean = np.tan(np.pi*(uni_sample - 0.5)).mean()
        mean.append(cauchy_mean)
    return mean
plt.hist(cauchy_sample_mean(10,size) , density =True,bins =50,range=[-10,10])
plt.show()
plt.hist(cauchy_sample_mean(100,size) , density =True,bins =50,range=[-10,10])
plt.show()
```
![image](https://hackmd.io/_uploads/ByNNgwGlR.png)
![image](https://hackmd.io/_uploads/S1gSevfl0.png)
å¯ä»¥çœ‹å‡ºæ¨£æœ¬å¹³å‡çš„åˆ†é…ä»é¡ä¼¼æŸ¯è¥¿åˆ†é…ï¼Œäº‹å¯¦ä¸Šé€™å€‹æ€§è³ªæ˜¯æˆç«‹çš„ï¼

